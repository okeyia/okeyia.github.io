<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>图处理相关论文 | okeyia</title><meta name="keywords" content="图重排序"><meta name="author" content="okeyia"><meta name="copyright" content="okeyia"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="图处理的相关论文 Benchmark：主要是对比别人的方法，这个方法不一定是最好的，但一定是最具有代表性且被广泛认可的（一种标准和规范）。其所用的数据就是benchmark data，其方法就是benchmark method，你提出的方法在benchmark data上得出的结果与benchmark method 的结果对比才知道你的方法是否足够好。 baseline:  主要关注自己提出的方法">
<meta property="og:type" content="article">
<meta property="og:title" content="图处理相关论文">
<meta property="og:url" content="http://okeyia.github.io/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/index.html">
<meta property="og:site_name" content="okeyia">
<meta property="og:description" content="图处理的相关论文 Benchmark：主要是对比别人的方法，这个方法不一定是最好的，但一定是最具有代表性且被广泛认可的（一种标准和规范）。其所用的数据就是benchmark data，其方法就是benchmark method，你提出的方法在benchmark data上得出的结果与benchmark method 的结果对比才知道你的方法是否足够好。 baseline:  主要关注自己提出的方法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/okeyia/PictBed/Blogimg/202211132313334.png">
<meta property="article:published_time" content="2022-05-15T03:43:57.000Z">
<meta property="article:modified_time" content="2022-11-17T09:45:45.113Z">
<meta property="article:author" content="okeyia">
<meta property="article:tag" content="图重排序">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/okeyia/PictBed/Blogimg/202211132313334.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://okeyia.github.io/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '图处理相关论文',
  isPost: true,
  isHome: false,
  isHighlightShrink: undefined,
  isToc: true,
  postUpdate: '2022-11-17 17:45:45'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="okeyia" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/okeyia/PictBed/Blogimg/202204171749626.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/books/"><i class="fa-fw fas fa-book-open"></i><span> Books</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/okeyia/PictBed/Blogimg/202211132313334.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">okeyia</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/books/"><i class="fa-fw fas fa-book-open"></i><span> Books</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">图处理相关论文</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-05-15T03:43:57.000Z" title="发表于 2022-05-15 11:43:57">2022-05-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-17T09:45:45.113Z" title="更新于 2022-11-17 17:45:45">2022-11-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87/">学术论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="图处理相关论文"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="图处理的相关论文"><a href="#图处理的相关论文" class="headerlink" title="图处理的相关论文"></a>图处理的相关论文</h2><blockquote>
<p>Benchmark：主要是对比别人的方法，这个方法不一定是最好的，但一定是最具有代表性且被广泛认可的（一种标准和规范）。其所用的数据就是benchmark data，其方法就是benchmark method，你提出的方法在benchmark data上得出的结果与benchmark method 的结果对比才知道你的方法是否足够好。</p>
<p>baseline:  主要关注自己提出的方法，比最原始最简单的方法出来的结果（参照物）。然后在这个基础上改进，增加各种组件，可以看出提升多少，通过baseline我们可以知道这个方法能不能work, 有多少提升。</p>
</blockquote>
<h3 id="STINGER-流图的高性能数据结构"><a href="#STINGER-流图的高性能数据结构" class="headerlink" title="STINGER: 流图的高性能数据结构"></a>STINGER: 流图的高性能数据结构</h3><hr>
<blockquote>
<p> Stinger的关键属性是具有偏斜度分布的语义图上的快速插入，删除和更新。提出了一种新的数据结构，能够达到比较快的更新速度。</p>
</blockquote>
<p>正是由于在偏斜度分布的图上面进行的，所有才会使用batch 更新的方法，在batch 中  STINGER 的后续实现首先对批次进行排序（通常一次 100,000 条边更新），以便将发生在特定顶点上的所有边更新与插入分开的删除分组在一起。</p>
<p><strong>为什么会有batch</strong>        </p>
<blockquote>
<p>在具有许多线程上下文和内存库的系统上，数据结构中的工作量或并行性通常不足以一次处理单个更新。为了解决这个问题，我们开始批量处理边缘更新。一个批次摊销了进入数据结构的成本，并提供了大量的独立工作要做。</p>
</blockquote>
<p><strong>实验：</strong> 为什么采用batch 更新的方式，当batch 的大小发生变化的时候，每秒更新的变数会提高？ 每秒更新的边的个数是怎么计算的？</p>
<blockquote>
<p>我们测量处理数据结构中每个边缘更新所花费的时间。我们测量了几个批次，并以每秒更新的形式报告了性能。</p>
</blockquote>
<p><strong>进一步的优化:</strong>  </p>
<blockquote>
<p>然而，在无标度图中，少数顶点将面临多次更新，而大多数顶点只有一次更新或根本没有更新。这种工作负载不平衡限制了我们可以利用的并行量，并迫使大多数线程等待少数线程完成。</p>
</blockquote>
<p>为了解决这个问题，跳过了对边进行分类，并行的处理每条边的插入。但是，处理同一顶点上的两个边更新事件会引入竞争条件，必须通过适当的同步来处理。 并没有在软件商提出解决办法，而是使用Cray XMT 是这个场景的完美系统。（Cray XMT（Cray eXtreme MultiThreading，[1] 代号 Eldorado[2]）是 Cray 公司基于第三代 Tera MTA 架构的可扩展多线程共享内存超级计算机架构，针对大型图问题），但是这个系统近10年没有消息。</p>
<p><strong>缺点:</strong>  在整篇论文中只提到了更新，没有考虑查询的情况。</p>
<h3 id="图处理工作负载内存层次结构的分析与优化"><a href="#图处理工作负载内存层次结构的分析与优化" class="headerlink" title="图处理工作负载内存层次结构的分析与优化"></a>图处理工作负载内存层次结构的分析与优化</h3><p align = "right" > ——2019年发表在 `HPCA` 上面的一篇论文<p/>

<hr>
<p>Analysis and Optimization of the Memory Hierarchy for Graph Processing Workloads。</p>
<p><strong>提出问题:</strong> 在微架构级别，性能受到单机内存图分析的内存子系统效率低下的限制。本文的目标是解决单机内存图分析的内存效率低下问题。</p>
<ol>
<li>首先，我们在模拟的多核架构上对图形处理工作负载进行深入的数据类型感知表征。我们分析 1) 乱序内核中的内存级并行性和 2) 缓存层次结构中的请求重用距离。我们发现，涉及不同应用程序数据类型的加载-加载依赖链构成了实现高内存级并行性的主要瓶颈。我们还观察到不同的图数据类型表现出异构的重用距离。因此，私有 L2 缓存对性能的贡献可以忽略不计，而共享 L3 缓存表现出更高的性能敏感度。</li>
<li>基于他们的观察结构，提出了 DROPLET，这是一种用于图应用程序的数据感知解耦 预取器。 DROPLET 根据其固有的重用距离以不同的方式预取不同的图形数据类型。</li>
</ol>
<p>这里通过 <code>Using Cycle Stacks to Understand Scaling Bottlenecks in Multi-Threaded Workloads</code> 介绍的方法，使用pagerank 在 orkut上面 得到了 ==Cycle stack==。  45% 的周期是 DRAM 绑定的停顿周期，而内核被充分利用而仅在 15% 的周期内没有停顿。</p>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20221114164748773.png" class="" title="image-20221114164748773">



<h4 id="一-借鉴的方向"><a href="#一-借鉴的方向" class="headerlink" title="一 借鉴的方向"></a>一 <strong>借鉴的方向</strong></h4><blockquote>
<p>现代 CPU 中使用的三种关键延迟容忍技术是 OoO 执行产生的 MLP、片上缓存(常说的三级cache)和预取。</p>
<p>OoO 执行依赖于重新排序缓冲区 (ROB) 向前看指令流，它可以在加载/存储队列、非阻塞缓存和 DRAM 中的并行性的帮助下支持多个正在运行的内存请求。</p>
</blockquote>
<p>这里主要参考优化的第一个方向,  通过两个特征来深入了解fig 1中的内存受限行为</p>
<ol>
<li>乱序执行中的内存级并行性（MLP）</li>
<li>cache层次结构中的重用距离</li>
</ol>
<blockquote>
<p>参考知乎: <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/28611947">https://www.zhihu.com/question/28611947</a></p>
<p><strong>instruction size 的概念:</strong>  标量乱序执行的核心就是不断的检查未来的指令，并从中发掘可以并行执行的指令，从而最终提升IPC。这个可以乱序执行的指令窗口叫做instruction window，这是衡量现代CPU性能的一个重要指标（举个不太恰当的例子，以前参加智能车比赛，摄像头看得越远理论上可以跑的越快）。</p>
<p>有两个关键因素影响着不断增加的instruction window size，一个是<strong>分支预测</strong>（branch prediction），准确的分支预测（目前的指标可以做到小于10MPKI，MPKI是没1000条指令中预测错误数目）可以保证绝大多数情况下instruction window 中的指令都是在正确的 程序路径 上。否则总是错误的预测执行（speculative execution）反而会降低IPC。 另外一个关键就是<strong>寄存器重命名和 reorder buffer 的大小</strong>。reorder buffer是实现乱序执行同时保持程序正确性和精确异常的关键，每条指令在译码之后会分配一个reorder buffer的entry，reorder buffer后面会按照指令顺序commit保证程序正确性。通常指令窗口大小（instruction window size）就是reorder buffer entry size，intel的几个关键节点的微架构的instruction window大小如下，Nehalem（45nm）128， Haswell（22nm）192，Sunny Cove（10nm）352。苹果的M1目前达到了~600（一部分原因是ARM不需要micro-ops译码，从而可以实现更高的instruction译码带宽8-wide）。</p>
</blockquote>
<h4 id="二-特征描述"><a href="#二-特征描述" class="headerlink" title="二 特征描述"></a>二 特征描述</h4><h4 id="2-1-指令窗口大小不是阻碍-MLP-的因素"><a href="#2-1-指令窗口大小不是阻碍-MLP-的因素" class="headerlink" title="2.1 指令窗口大小不是阻碍 MLP 的因素"></a>2.1 指令窗口大小不是阻碍 MLP 的因素</h4><ol>
<li>之前有论文说 ROB是影响MLP的主要因素，他们通过扩大指令窗口的大小，发现==带宽==并没有明显的增加。</li>
<li>第二个是发现加速比也没有明显的增加。</li>
</ol>
<h4 id="2-1-2-负载依赖链阻止实现高-MLP"><a href="#2-1-2-负载依赖链阻止实现高-MLP" class="headerlink" title="2.1.2 负载依赖链阻止实现高 MLP"></a>2.1.2 负载依赖链阻止实现高 MLP</h4><blockquote>
<p>laod-load dependency chains prevent achieving high MLP</p>
</blockquote>
<p>为了理解为什么大的ROB不能提升内存间的并行, 我们跟踪 ROB 中加载指令的依赖关系, MLP 受固有的应用程序级依赖特性的限制。</p>
<h4 id="2-1-3-图属性数据是依赖链中的消费者"><a href="#2-1-3-图属性数据是依赖链中的消费者" class="headerlink" title="2.1.3 图属性数据是依赖链中的消费者"></a>2.1.3 图属性数据是依赖链中的消费者</h4><p>平均而言，我们发现图形属性数据主要是消费者（53.6%）而不是生产者（5.9%）。发布(issuing)图形属性数据加载被延迟并且不能并行化，因为它必须依赖生产者加载来计算地址。</p>
<h4 id="2-1-4-私有-L2-缓存表现出可忽略的性能敏感性，而共享-LLC-表现出更高的性能敏感性"><a href="#2-1-4-私有-L2-缓存表现出可忽略的性能敏感性，而共享-LLC-表现出更高的性能敏感性" class="headerlink" title="2.1.4 私有 L2 缓存表现出可忽略的性能敏感性，而共享 LLC 表现出更高的性能敏感性"></a>2.1.4 私有 L2 缓存表现出可忽略的性能敏感性，而共享 LLC 表现出更高的性能敏感性</h4><img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20221116195030491.png" class="" title="image-20221116195030491">

<p>如图 4a 所示，我们将 LLC 大小从 8MB 更改为 64MB，并找到 17.4%（最大 3.25X）性能改进的最佳点，LLC 容量增加 4 倍。平均 LLC MPKI（每千克指令缺失）从基线中的 20 减少到 16 (16MB) 到 12 (32MB) 到 10 (64MB)。相应的加速比分别为 7%、17.4% 和 7.6%。最佳点是在降低的未命中率和更大的 LLC 访问延迟之间取得平衡。</p>
<p>图 4b(i) 显示 L2 命中率（在基线中已经非常低，为 10.6%）在容量增加 2 倍后增加到仅 15.3%，而集关联性增加 4 倍没有影响（命中率升至仅 10.9%）。</p>
<p>图 4b(ii) 显示系统性能对不同的 L2 缓存配置（容量和集合关联性）表现出很小的敏感性。最左边的条表示没有私有 L2 缓存的架构，与 256KB 缓存相比没有减速。因此，没有私有 L2 缓存的架构同样适用于图形处理。</p>
<h4 id="2-1-5-属性数据是-LLC-容量的主要受益者"><a href="#2-1-5-属性数据是-LLC-容量的主要受益者" class="headerlink" title="2.1.5 属性数据是 LLC 容量的主要受益者"></a>2.1.5 属性数据是 LLC 容量的主要受益者</h4><p>为了了解哪种数据类型受益于更大的 LLC，图 4c 显示了对于每种数据类型，最终从 DRAM 获取数据的内存引用的百分比。属性数据下降最为明显.</p>
<h4 id="2-1-6-图结构缓存行在所有数据类型中具有最大的重用距离。-Graph-property-cacheline-具有比-L2-缓存服务更大的重用距离"><a href="#2-1-6-图结构缓存行在所有数据类型中具有最大的重用距离。-Graph-property-cacheline-具有比-L2-缓存服务更大的重用距离" class="headerlink" title="2.1.6 图结构缓存行在所有数据类型中具有最大的重用距离。 Graph property cacheline 具有比 L2 缓存服务更大的重用距离"></a>2.1.6 图结构缓存行在所有数据类型中具有最大的重用距离。 Graph property cacheline 具有比 L2 缓存服务更大的重用距离</h4><img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20221116195837647.png" class="" title="image-20221116195837647">

<blockquote>
<p>Graph structure cacheline has the largest reuse distance among all the data types. Graph property cacheline has a larger reuse distance than that serviced by the L2 cache.</p>
</blockquote>
<p>为了进一步了解 L2 和 L3 缓存的不同性能敏感性，我们按应用程序数据类型分解内存层次结构使用情况，如图 7 所示。</p>
<ol>
<li>在大多数基准测试中，对结构数据的访问由 L1 缓存和 DRAM 提供服务，这表明 L1 中丢失的缓存行是在遥远的过去引用的缓存行，因此它已从 L2 和 L3 缓存中被逐出。 重用距离超出 LLC 的服务能力这一事实解释了为什么更大的 LLC 无法显着降低图 4c 中片外结构访问的比例。</li>
<li>另一方面，L1 缓存中丢失的大部分属性数据加载不能由 L2 缓存提供服务，但可以由 LLC 和 DRAM 提供服务。总的来说，LLC 在服务属性访问方面比结构访问更有用。因此，property cacheline 的重用距离相对较小，但仍大于 L2 缓存捕获的重用距离</li>
<li>最后，图 7 提供的证据表明，对中间(顶点)数据的访问主要是 L1 缓存和 LLC 中的片上缓存命中。</li>
</ol>
<p>**最终: ** 三种数据类型的重用距离解释了为什么私有 L2 缓存无法为内存请求提供服务并且显示出微不足道的好处。</p>
<h4 id="三-总结与机遇"><a href="#三-总结与机遇" class="headerlink" title="三 总结与机遇"></a>三 总结与机遇</h4><p>图形分析中内存限制的停顿行为是由两个问题引起的：</p>
<ol>
<li>不同数据类型的异构重用距离导致密集的 DRAM 访问以检索结构和属性数据。</li>
<li>由于负载依赖链导致的低 MLP，限制了重叠 DRAM 访问的可能性</li>
</ol>
<hr>
<h3 id="Graphfire：为图处理协同获取、插入和替换策略"><a href="#Graphfire：为图处理协同获取、插入和替换策略" class="headerlink" title="Graphfire：为图处理协同获取、插入和替换策略"></a>Graphfire：为图处理协同获取、插入和替换策略</h3><blockquote>
<p>已经开发了专门的图形定制预取机制、处理器设计和内存层次结构引擎，以容忍此类访问的长时间延迟. ==但是==这些方法要么过于占用带宽，要么需要进行侵入性硬件更改，从而抑制通用计算的灵活性，要么依赖于限制真正加速的软件预处理。</p>
</blockquote>
<p>这项工作引入了 Graphfire，这是一种灵活的内存层次结构方法，可以学习图形处理中的不同访问模式，并利用专门的获取、插入和替换优化的协同作用来解决有问题的间接访问，而无需依赖软件或 ISA 支持。</p>
<h4 id="一-已有工作的问题"><a href="#一-已有工作的问题" class="headerlink" title="一 已有工作的问题"></a>一 已有工作的问题</h4><ul>
<li>大量工作涉及缓存管理技术 这些技术都没有考虑图形应用程序的特定访问模式，而是专注于那些在更常规的工作负载中众所周知和常见的模式，例如streaming、strided、thrashing、mixed 等。因此，当这些技术应用于图形分析时，这些技术产生的任何硬件开销都被浪费了。</li>
<li>GRASP [15] 提出了针对图分析的领域专业化 LLC 管理的第一步，但会产生基于度的图重新排序的软件预处理成本 [14]。软件预处理使得该技术对于大图不太实用，例如在许多应用场景中，输入图只被处理一次 [5]，或者甚至没有完全遍历图，例如在搜索算法中。</li>
</ul>
<p>据我们所知，对于图形应用程序<strong>，不存在可以在没有软件支持的情况下学习和优化其访问模式的内存层次结构方法</strong>。</p>
<h4 id="二-我们的工作"><a href="#二-我们的工作" class="headerlink" title="二 我们的工作"></a>二 我们的工作</h4><p>以优化图形应用程序的缓存性能为目标，我们的工作做出以下==关键观察==：</p>
<ol>
<li>内存层次结构必须专门针对有问题的间接访问（ problematic indirect accesses）来缓解它们的瓶颈。</li>
<li>要与软件无关，==轻量级机制==必须自动识别 PIA，这可以在每个指令的基础上实现。</li>
<li>虽然 PIA 是不规则的，但其中的一个子集具有很高的重用性，因此 LLC 必须保留它们。</li>
</ol>
<p><strong>所用的方法：</strong></p>
<p>鉴于这些观察结果，本文提出了 Graphfire，这是一种灵活的、基于硬件的内存层次结构方法</p>
<ul>
<li>了解图形应用程序中何时出现 PIA</li>
<li>通过定制的获取、插入和替换策略优化它们的性能。</li>
</ul>
<h4 id="三-MOTIVATION"><a href="#三-MOTIVATION" class="headerlink" title="三 MOTIVATION"></a>三 MOTIVATION</h4><h5 id="3-1-内存访问模式"><a href="#3-1-内存访问模式" class="headerlink" title="3.1 内存访问模式"></a>3.1 内存访问模式</h5><p>图形应用程序因数据遍历引起的不规则内存访问而臭名昭著。最先进、高效的算法实现通过两个嵌套的内核循环迭代地执行图形遍历，并利用压缩稀疏行 (CSR) 格式将输入数据集有效地存储为一维密集数组 [45]。</p>
<p>指针间接访问发生在顶点属性数组中，该数组存储每个顶点的结果，例如距离或等级 [6]。 CSR 数组存储图形信息，例如顶点和边缘位置，但经常和/或不经常访问。</p>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20221117103412358.png" class="" title="image-20221117103412358">

<p><strong>上图是图处理内核的伪代码</strong>， 外部循环（第 2-5 行）遍历顶点工作列表（针对当前算法迭代），而内部循环（第 6-13 行）分析当前顶点的邻居以潜在地更新它们的数据，具体取决于算法的目标。如果更新了邻居，则将其添加到下一次算法迭代的工作列表中（第 12-13 行）。该算法在工作列表为空（第 1 行）时终止，即图已遍历且更新已稳定。</p>
<p>==外循环中的指令==相对于内循环中的指令很少出现，特别是对于具有高边顶点比的图。访问是流式的或间接的（这里的访问是流失的是什么意思? 一直访问吗?） </p>
<h6 id="3-1-1-Infrequent-Streaming"><a href="#3-1-1-Infrequent-Streaming" class="headerlink" title="3.1.1 Infrequent, Streaming"></a>3.1.1 Infrequent, Streaming</h6><p>这些访问出现在外部 for 循环或内部循环的条件内部，并具有流式行为。如图 1 的第 2 行所示，该算法遍历顶点工作列表并以流方式加载每个顶点索引。然而，这种访问并没有表现出良好的局部性，因为在内核的内部循环中，每次访问都可以被多次内存访问分开。</p>
<h6 id="3-1-2-Infrequent-Indirect"><a href="#3-1-2-Infrequent-Indirect" class="headerlink" title="3.1.2 Infrequent, Indirect"></a>3.1.2 Infrequent, Indirect</h6><p>这些指针间接访问出现在外部 for 循环或内部循环中的条件。当前顶点 v 索引到 vertex_ptr 数组以加载其邻居列表索引（第 4-5 行）并确定内循环迭代次数。对起始索引的第一个加载（第 4 行）是指针间接的，而第二个（第 5 行）具有局部性。另一个间接访问来自有条件地更新顶点属性数据（第 10 行）。但是，这些不规则访问不会对性能产生重大影响，因为它们很少发生。</p>
<h6 id="3-1-3-Primary-Streaming-Accesses-PSAs"><a href="#3-1-3-Primary-Streaming-Accesses-PSAs" class="headerlink" title="3.1.3 Primary Streaming Accesses (PSAs)"></a>3.1.3 Primary Streaming Accesses (PSAs)</h6><p>这些访问发生在内部 for 循环的关键路径中，并执行流式加载或存储到相邻索引（第 6 行）。这些是真正的流式访问，同时展示了时间和空间局部性，并且它们经常且定期发生。因此，它们是缓存友好的，不会造成数据供应瓶颈。我们将这些访问称为主要流访问 (PSA) 来描述这些特征。</p>
<h6 id="3-1-4-Primary-Indirect-Accesses"><a href="#3-1-4-Primary-Indirect-Accesses" class="headerlink" title="3.1.4 Primary Indirect Accesses"></a>3.1.4 Primary Indirect Accesses</h6><p>这些访问也发生在内部 for 循环的关键路径中，并执行指针间接加载（在第 8 行中突出显示）到顶点属性数组，以便为给定的邻居加载数据。由于这些间接访问发生在每个循环迭代中，并且具有导致性能成本的较长延迟，因此它们是图应用程序的数据供应瓶颈的原因。我们将它们称为主要间接访问 (PIA)，并在下一节中详细介绍它们的特性。</p>
<h5 id="3-2-The-Problems-with-PIAs"><a href="#3-2-The-Problems-with-PIAs" class="headerlink" title="3.2 The Problems with PIAs"></a>3.2 The Problems with PIAs</h5><img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20221117112612220.png" class="" title="image-20221117112612220">

<p>顶点属性数组是 PIA 的主要来源，但是分析footprint, 发现只占应用程序总内存占用量的很小一部分。（这里的内存占用是指什么？）然后做实验发现了<strong>尽管 PIA 的数据占用空间相对较小，但它们本身占应用程序访问总数的很大一部分。</strong> 当应用程序遍历图形时，必须对顶点属性数组进行频繁更新。将不规则性与频率相结合会产生==内存延迟性能瓶颈==。</p>
<p>图 3 将总内存延迟分解为 PIA 与其他内存访问。在所有应用程序和输入中，PIA 的延迟平均占总内存延迟的 88%，构成了主要的应用程序性能瓶颈。因此，必须在内存层次结构中进行创新，以解决这些引用所表现出的缺乏规律性和局部性的问题。</p>
<h6 id="3-2-1-Lack-of-Locality"><a href="#3-2-1-Lack-of-Locality" class="headerlink" title="3.2.1 Lack of Locality"></a>3.2.1 Lack of Locality</h6><p>不幸的是，PIA 的不规则性导致它们表现出较差的局部性。对于上述应用程序/输入组合，平均而言，在 64B 逐出缓存行中有 54.1 (L1)、59.9 (L2) 和 59.5 (L3) 字节未使用。因此，为 PIA 获取整个缓存行的数据是一种浪费。这个没有具体的数据来表示？ 是怎么计算出来的？</p>
<h6 id="3-2-2-访问模式之间的干扰"><a href="#3-2-2-访问模式之间的干扰" class="headerlink" title="3.2.2 访问模式之间的干扰"></a>3.2.2 访问模式之间的干扰</h6><p>同一缓存集中不同内存访问模式的共存会损害性能。 PIA 可能会被 PSA 或其他不频繁访问逐出，从而导致多次冲突未命中，尤其是具有高重用性的 PIA 应保留在 LLC [6] 中。由于其不规则性和频率，PIA 是 LLC 中的主要驱逐对象。平均而言，21% 的驱逐是由其他类型的访问造成的。消除这种干扰可以提高 PIA 的缓存性能。</p>
<h6 id="3-2-3-变量-PIA-重用"><a href="#3-2-3-变量-PIA-重用" class="headerlink" title="3.2.3 变量 PIA 重用"></a>3.2.3 变量 PIA 重用</h6><img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20221117112845107.png" class="" title="image-20221117112845107">

<p>图 4 展示了 BFS 在 Kronecker 网络上运行时的 PIA 访问和重用直方图。这捕捉了许多现实世界数据集的幂律趋势。图 4a 显示很少有顶点被频繁访问，而大多数顶点很少被访问。具有高重用性的顶点不应被低重用性 PIA 逐出。图 4b 说明了 PIA 重用距离的可变性。许多被重用，但大多数重用距离太长（相对于缓存关联性）以防止 PIA 被逐出。因此，PIA 的替换策略应适应顶点特征。</p>
<h3 id="用于图形分析的领域专用缓存管理"><a href="#用于图形分析的领域专用缓存管理" class="headerlink" title="用于图形分析的领域专用缓存管理"></a>用于图形分析的领域专用缓存管理</h3><p>Domain-Specialized Cache Management for Graph Analytics                                              ——HPCA 20</p>
<hr>
<h3 id="多核系统中处理内存干扰的技术-17-浙大"><a href="#多核系统中处理内存干扰的技术-17-浙大" class="headerlink" title="多核系统中处理内存干扰的技术 17 浙大"></a>多核系统中处理内存干扰的技术 17 浙大</h3><p>在多核系统中，同时运行的多个应用程序相互竞争访问共享的资源，如互连、高速缓 存和内存等。如果对可用的共享高速缓存容量和内存带宽的管理不恰当的话，不同应用程 序相互干扰，严重影响对方的运行。</p>
<blockquote>
<p>例如，在内存处，应用程序原有的行缓冲命中率和阵列级并行度会受到破坏，同时请求在读写队列中等待的时间也会因竞争激烈程度的加剧而 大幅增加；在高速缓存处，不同应用程序可能相互驱逐对方在高速缓存中的块，导致原有 命中率的破坏。</p>
</blockquote>
<p>请求在内存处的时延主要包含在队列中的等待时间和在DRAM中执行时间，这两 个部分受内存调度算法的影响特别大。</p>
<h4 id="创新-基于动态多层次优-先级的内存访问调度算法-DMPS"><a href="#创新-基于动态多层次优-先级的内存访问调度算法-DMPS" class="headerlink" title="创新: 基于动态多层次优 先级的内存访问调度算法 DMPS"></a>创新: 基于动态多层次优 先级的内存访问调度算法 DMPS</h4><p>识别应用程序的内存访问调度算法一般 由三部分组成：</p>
<ol>
<li>检测应用程序的内存访问特征；</li>
<li>基于内存访问特征来将应用程序分 类，以至于易受干扰的应用程序拥有更高的优先级；</li>
<li>选择优先级最高的就绪命令来执 行。</li>
</ol>
<h4 id="一-DRAM-访问过程"><a href="#一-DRAM-访问过程" class="headerlink" title="一 DRAM 访问过程"></a>一 DRAM 访问过程</h4><p>行缓冲是阵列中感应放大器单元的集合，是DRAM和内存控制器交互的接口。一般来说，行缓冲的大小为 <code>2-16KB</code>。</p>
<p>在行缓冲的行没有被关掉之前，行缓冲类似于高速缓存， 命中的话可减小访问延迟。到DRAM的访问可分为三步：</p>
<ol>
<li>激活命令，在目标阵列中打 开目标行，将其内容转移到行缓冲中</li>
<li>读写命令，在行缓冲中访问目标列；</li>
<li>预充电 命令，将行缓冲中的内容写回阵列的数组，关闭行缓冲的行。</li>
</ol>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220727095026744.png" class="">

<h4 id="二-页管理策略"><a href="#二-页管理策略" class="headerlink" title="二 页管理策略"></a>二 页管理策略</h4><p>页管理策略管理行缓冲中感应放大器的操作，基本的策略有两种：开页策略和关页策 略</p>
<p>在开页策略中，<strong>行缓冲只在没 有到打开行的访问且有到其他行的访问时才关闭。</strong>开页策略的重要假设是一旦某一行数 据移到行缓冲中，那么在不久的将来该行的其他列会被访问，即偏爱到同一行的访问，适 合于空间局部性好的应用程序。访问类型主要是行命中和行冲突，对于行命中率高的程 序，开页策略能大幅提高性能。</p>
<p>在关页策略中，行缓冲在每个访问结束后都会关闭，访问 类型只有行关闭。关页策略适合到不同行的随机访问，同时每个访问的时延一样，有利于 带宽分配和实时控制</p>
<h4 id="三-地址映射机制"><a href="#三-地址映射机制" class="headerlink" title="三 地址映射机制"></a>三 地址映射机制</h4><p>地址映射机制负责将系统的内存地址空间映射到DRAM的逻辑结构中，具体来说，地 址映射机制将访问的物理地址转换成通道、排、阵列、行、列等值，具体化数据的放置位 置，对于性能有巨大的影响。当某块放置在某阵列中，下一块可以放置在同一行中、或者 同一阵列的下一行中、或者同一排的下一阵列中、或者同一通道的下一排中、又或者下一 通道中，所以地址映射机制决定了内存系统中可利用的并行度。</p>
<p>常见的地址映射机制有两 种：块交叉(Cacheline Interleaving)和行交叉(Row Interleaving)，如图2．3所示。行交叉将 连续的块放置在同一行中，试图最大化行缓冲命中率，适用于开页策略；而块交叉将连续 的块分散到不同的通道、排和阵列中，从而最大化内存访问并行度，适用于关页策略。最 小化开页(Minimalist Open Page) 则在两者之间做了权衡，通过少量的行命中实现开页 的增益，同时提高并行度来防止访问饥饿的现象和保证公平性。</p>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220727103932041.png" class="" title="image-20220727103932041">



<hr>
<h3 id="基于依赖感知的动态有向图处理加速器-20-华科"><a href="#基于依赖感知的动态有向图处理加速器-20-华科" class="headerlink" title="基于依赖感知的动态有向图处理加速器 20 华科"></a>基于依赖感知的动态有向图处理加速器 20 华科</h3><p>图算法通常需要对整个图进行反复迭代处理，不断地更新图顶点状态值，最终使 得所有图顶点状态值都不再发生改变，才停止迭代处理过程。因此，现有动态图处理 系统通常采用图迭代模型对最新图镜像执行增量计算。</p>
<p>目前的图迭代模型通常包括 批量同步迭代方法（Bulk Synchronous Parallel，BSP）模型和异步迭代模型。</p>
<h4 id="1-同步迭代模型"><a href="#1-同步迭代模型" class="headerlink" title="1 同步迭代模型"></a>1 同步迭代模型</h4><p>如图 1.3 所示，同步 BSP 迭代模型使用同步屏障机制将整个执行流程划分为数 个迭代周期。在每轮迭代中，基本并行处理单元被分配给各线程并行处理，通过对图 顶点及其相连的边执行运算以获得图顶点状态值（即，算法结果）。然而，由于同步 屏障的限制，各个图顶点都只能使用其前序图顶点在前一轮迭代中的旧状态值来计 算各自的新状态值，所有图顶点都完成各自的计算才能开始新的一轮迭代。迭代周期 交错进行，直到达到收敛状态。当 图处理系统并行处理图数据时，图数据被划分为并 行块并且分配给不同的处理单元。不同并行块之间通过共享内存或者消息通信机制 进行数据交换。</p>
<p>近年来，软件图处理系统和图加速器提出许多图划分方法，运行时负 载均衡策略和访存优化策略以提高每一轮迭代中图处理系统的吞吐率，并且已经达 到很好的效果。然而，受限于同步 BSP 迭代模型，图顶点的状态值在每轮迭代中只 能到达其直接后代图顶点，造成图顶点状态值的缓慢传播。因此，现有的同步 BSP 迭代模型不能有效支持动态图增量计算对低延迟和实时性的要求。</p>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220801201253986.png" class="" title="image-20220801201253986">

<h4 id="2-异步迭代模型"><a href="#2-异步迭代模型" class="headerlink" title="2 异步迭代模型"></a>2 异步迭代模型</h4><p>不同于同步 BSP 迭代模型，如图 1.4 所示，异步迭代模型消除了同步屏障，当 前迭代计算得到的结果可以立即用于同一迭代中其它图顶点的状态值更新。因此，在 采用异步迭代模型的情况下，动态图中的图顶点的状态值传递速度通常快于同步 BSP 迭代模型，能够加快迭代收敛速度。</p>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220801201340527.png" class="" title="image-20220801201340527">

<p>然而在异步迭代中，同时处理相邻图顶点会导致共享内存的读写冲突。为保证 异步迭代的顺序一致性，一些工作[20]对共享数据加锁以避免相邻图顶点的同时处 理。这导致大量锁开销，严重影响了大度图顶点的执行效率，对异步迭代处理性能 产生影响。为了减少原子开销，一些子图中心的异步迭代方法针对系统资源数 量，将有向图划分为图数据块并且分配图数据块给各个并行处理单元。不同图数据 块被并行地处理，通过消息通信机制同步不同图数据块之间的图顶点状态值。在各 个数据块内部，图顶点被串行地，异步地执行以快速传递图顶点状态值。</p>
<h4 id="3-设计动机"><a href="#3-设计动机" class="headerlink" title="3 设计动机"></a>3 设计动机</h4><blockquote>
<p>现有的同步 BSP 迭代模型和异步迭代模型能够提升有向图算法的并行度， 广泛应用于多种图处理系统和图加速器中。然而，由于动态有向图增量计算对实时性 要求极高，现有的迭代模型仍然面临着收敛速度缓慢和冗余数据计算和访问等问题， 无法满足用户对动态有向图处理的实时性需求。</p>
</blockquote>
<p>在动态有向图增量计算中，受到动态图变化影响的图顶点会沿着有向路径不断 地传递各自的状态值，因此，动态有向图处理的实时性直接受到图顶点的状态值传递 速度的影响。对于迭代有向图算法，每个图顶点都需要读取其前序图顶点的状态值以 重复更新自身的最新状态值，直到迭代收敛为止。但是，当在现有平台上并行执行图 算法时，大多数图顶点和它的前序图顶点被多个并行处理单元同时处理，在每轮迭代 中根据其前序图顶点的过时状态值更新以更新自身的状态值。结果，当使用现有的同 步/异步迭代模型时，活跃图顶点的最新状态值只能够缓慢地沿着有向路径传播到其 他的图顶点，并且根据其它图顶点的陈旧状态值来重复进行计算以更新自身。这不仅 浪费了大量时间用于处理冗余图顶点，还需要高额访存开销以反复加载这些图数据。</p>
<h5 id="3-1-同步BSP迭代过程"><a href="#3-1-同步BSP迭代过程" class="headerlink" title="3.1 同步BSP迭代过程"></a>3.1 同步BSP迭代过程</h5><p>如图 2.1 所示，动态图处理系统采用同步 BSP 迭代模型执行增量计算。在初始 状态时，动态有向图中所有的图顶点都达到收敛状态，并且相应的计算结果被维护。 当动态有向图结构发生变化时，例如，新增加了指向𝑣&amp;、𝑣’和𝑣(的边，需要重新进行 增量计算以获得最新图镜像的计算结果。图处理系统将激活图顶点，并且按照同步 BSP 迭代方式处理这些图顶点。在第一轮同步迭代中，𝑣&amp;的状态值首先被传递给其 后代图顶点𝑣’。然而，由于同步屏障，活跃图顶点的新状态值不能够立即被同一轮同 步迭代中的其它图顶点使用，因此，𝑣’不能够立即使用𝑣&amp;的状态值，也无法将𝑣&amp;的状态值立即传递给其后代图顶点𝑣(。当新的一轮迭代开始后，𝑣’才将自己接收到的𝑣&amp;传 递来的状态值后得到的结果传递给𝑣(。按照这种方式，在图数据块 P2 中的图顶点需 要至少三轮同步迭代才能将状态值传递给其它图数据块，造成了大量的冗余图数据 计算和访问，导致图顶点状态值缓慢地在有向图中传播。</p>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220801204827566.png" class="" title="image-20220801204827566">

<h5 id="3-2-异步迭代模型"><a href="#3-2-异步迭代模型" class="headerlink" title="3.2 异步迭代模型"></a>3.2 异步迭代模型</h5><blockquote>
<p>由于传统的 round-robin 异步迭代模型[21]通常按照图顶点索引次序依次异步串行地处理活跃图顶 点，这忽略了有向图结构本身的更新依赖关系，导致低拓扑顺序的图顶点比高拓扑顺 序的图顶点先被处理。已经处理过的图顶点的状态值只能在下一轮异步迭代中才能 再次被处理，因此，在传统的异步迭代模型下，图顶点状态值仍然缓慢地在有向图中 传播，造成冗余图顶点更新。</p>
</blockquote>
<p>如图 2.2 所示，当动态图处理系统采用 round-robin 异步迭代模型执行增量计算 时，在其中一轮异步迭代过程中，一条已经被动态图改变量激活的有向路径（即， 𝑣&amp; → 𝑣’ → 𝑣(）上的图顶点可能被分配任意的图顶点索引次序，当图数据块 P2 被调 度处理时，其中的图顶点被按照图顶点索引次序依次异步串行地更新图顶点状态值。 然而，在每一轮异步迭代中，图顶点按照𝑣(，𝑣’，𝑣&amp;的顺序依次向后代图顶点传递各 自的图顶点状态值。由于已经处理过的图顶点的状态值只能在下一轮异步迭代中才 能再次被更新，因此在本轮异步迭代中，尽管图顶点可以立即使用并传递其前序图顶 点的状态值，但是其后代图顶点只能在下一轮迭代才能被重新更新。由于有向路径 (𝑣&amp; → 𝑣’ → 𝑣( → 𝑣))中包含 3 条边，因此至少需要进行三轮异步迭代（包含线程间同 步），图数据块 P2 才能将图顶点状态值完全传递给其它图数据块。因此，无论是同 步迭代模型，还是传统异步迭代模型，都 无法有效感知动态有向图的更新依赖关系，无法利用拓扑结构来加速动态有向图增量计算中的图顶点状态值传递。</p>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220801204945409.png" class="" title="image-20220801204945409">









<h4 id="二-实验部分"><a href="#二-实验部分" class="headerlink" title="二 实验部分"></a>二 实验部分</h4><blockquote>
<p>The Aspen interface is an extension of Ligra’s interface. It includes the full Ligra interface-vertexSubsets, edgeMap, and various other functionality on a fixed graph. </p>
<p>On top of Ligra, we add a set of functions for updating the graph - in particular, for inserting or deleting sets of edges or sets of vertices. We also add a flat-snapshot function.  所有处理和更新的函数都是工作在固定大小的、不变的图镜像版本上面.</p>
</blockquote>
<h3 id="四-RisGraph-A-Real-Time-Streaming-System-for-Evolving-Graphs-to-Support-Sub-millisecond-Per-update-Analysis-at-Millions-Ops-s"><a href="#四-RisGraph-A-Real-Time-Streaming-System-for-Evolving-Graphs-to-Support-Sub-millisecond-Per-update-Analysis-at-Millions-Ops-s" class="headerlink" title="四 RisGraph: A Real-Time Streaming System for Evolving Graphs to Support Sub-millisecond Per-update Analysis at Millions Ops/s"></a>四 RisGraph: A Real-Time Streaming System for Evolving Graphs to Support Sub-millisecond Per-update Analysis at Millions Ops/s</h3><blockquote>
<p>单调算法（例如可达性和最短路径）在实时分析中广泛使用，以获得 both static and temporal insights(见解)，并且可以通过增量计算加速。现有的 streaming system 采用增量计算模型，并实现低潜伏期或高吞吐量，但不能两者兼而有之。</p>
<p>RisGraph 通过局部数据访问和更新的并行性解决挑战.</p>
<p>单调算法在不断 evolving graph 中经常使用,  其中包括可及性，广度搜索，最短路径，连接的组件（和最小/最大标签传播(Connected Components, and Min/Max Label Propagation)。它需要扫描大量数据甚至整个图表，以重新计算不断发展的图的每个快照上的单调算法。增量计算的想法可以通过利用先前的结果来减少冗余计算来加速单调算法。</p>
</blockquote>
<p>RisGraph 单边更新， 与批处理相比，Per-Update分析对延迟友好，产生最新结果，并提供最准确，最详细的信息。它只留下一个开放的问题：如何在per-update analysis 中提供高吞吐量。</p>
<h4 id="一-motivation"><a href="#一-motivation" class="headerlink" title="一 motivation"></a>一 motivation</h4><p>已有的解决方案：<code>Kickstarter</code> 和 <code>Differential Dataflow</code> 是最先进的代表, <code>Kickstarter</code> 提出了单调算法的增量图计算模型，而<code>Differential Dataflow</code>则呈现了无图形意识的广义增量模型。</p>
<p><strong>不足:</strong>  如果<code>Kickstarter</code> 和 <code>Differential Dataflow</code> 每次更改图形时都（Per Update Analysis）进行分析，则每秒只能处理大约1000个更新。they rely on batching to trade latency for higher throughput(以延迟换吞吐量), benefiting from larger concurrency and lower overheads.  此外，它们提供了批处理模式以进一步优化吞吐量，从而降低了分析的频率，并仅对每批批次产生一个汇总的最终结果。</p>
<p><strong>不足举例:</strong>  我们以2010年Twitter-2010 [48]的范围进行广度优先搜索（<code>BFS</code>）。为了满足20 ms延迟需求（实时分析[65]），这些系统的吞吐量仅为1K OPS/s。为了提供 100K OPS/s的吞吐量，它们需要批量超过 <code>20k</code> 的更新，并且平均处理时间增长到150毫秒以上。因此，现有的流图系统不能同时通过批处理满足延迟和吞吐量要求。尽管如此，批处理模式还是整个更新，跳过了中间状态，这些状态在某些情况下可能有用，例如财务欺诈检测和交易综合性。</p>
<p><strong>单边更新的挑战：</strong> </p>
<ol>
<li>无法像 batch 更新那样，均摊开销</li>
<li>高吞吐量和低延迟 的目标要求系统有效地进行两种工作负载。修改图表时，它需要将每个更新应用于数据结构，并提供更新的图表，以便在短时间内进行分析。为了启用每个更新的实时分析，系统需要一个图形感知的设计，以利用单个更新的局部性，而不是利用整个图形扫描的典型技术。此外，它需要一种新的机制才能使平行性进行PerDate Processing，这对于在不批处理的情况下实现高吞吐量也很重要。</li>
</ol>
<p><strong>guiding idea：</strong></p>
<ol>
<li>局部数据访问的想法来自以下观察结果：图形流系统的常用图形感知技术仍然需要不必要的整个图形扫描[43，54，68，77]。如果我们仅通过访问受更新影响的必要顶点来避免这些扫描，我们将获得更好的性能，因此我们建议使用称为索引的邻接列表和稀疏阵列的数据结构来启用本地化访问。</li>
<li>我们通过并行处理更新（跨越并行性），同时维护应用程序的每个语义，进一步改善了吞吐量。我们提出了一种算法，以识别可以并行执行的更新，并一个一个更新以保持低延迟，以及原子能，隔离和Per Update分析的正确性。</li>
</ol>
<h4 id="二-实验结果"><a href="#二-实验结果" class="headerlink" title="二 实验结果"></a>二 实验结果</h4><p>四种算法： BFS、SSSP、SSWP、WCC</p>
<p>我们首先加载90％的边缘，选择10％的边缘作为从加载边缘的删除更新，然后将其余（10％）边缘视为插入更新。删除和更新占比1:1。</p>
<p>与<code>Kickstarter</code>相比，<code>Risgraph</code>的性能改进主要是我们本地化数据访问的信用（第3节）。 <code>Risgraph</code>的表现优于差异数据流，这主要是由于专门的图形感知引擎和增量模型。例如，与<code>Risgraph</code>相比，在Twitter-2010上重新计算<code>BFS</code>的处理时间为78倍。</p>
<h4 id="三-相关工作"><a href="#三-相关工作" class="headerlink" title="三 相关工作"></a>三 相关工作</h4><ol>
<li><code>Kickstarter</code> 通过追踪依赖关系和修剪近似值，为单调算法提供了正确的增量计算。 </li>
<li><code>Grapu</code>  by components-based classification and in-buffer <code>precomputation</code> 加速批处理单调算法。</li>
<li><code>Graphin</code> 结合了一个I-GAS模型，该模型会逐步处理固定尺寸的更新。</li>
<li><code>GraphBolt</code> 提出了一个通用的增量模型来 处理 non-monotonic algorithms like Belief Propagation,，但比单调算法的 <code>kickstarter</code> 涉及更多的开销。</li>
</ol>
<h3 id="五-GraphPulse-An-Event-Driven-Hardware-Accelerator-for-Asynchronous-Graph-Processing"><a href="#五-GraphPulse-An-Event-Driven-Hardware-Accelerator-for-Asynchronous-Graph-Processing" class="headerlink" title="五 GraphPulse: An Event-Driven Hardware Accelerator for Asynchronous Graph Processing"></a>五 <code>GraphPulse</code>: An Event-Driven Hardware Accelerator for Asynchronous Graph Processing</h3><p>​                                                                                                                                                                                                ——2020 MICRO</p>
<blockquote>
<p>大规模的图处理带来了一些问题:</p>
<ol>
<li>首先，内存密集型处理会给内存系统带来压力, 和计算型系统相比, 内存占用更大, 频率更高. 大型内存占用还会导致内存带宽瓶颈并加剧长期访问延迟.</li>
<li>其次，由于顶点的并发更新，大多数计算模型中访问共享图状态的同步开销很高.</li>
<li>跟踪活动顶点或者边缘的开销很大, 这种跟踪是必不可少的，因为计算是不规则的，而在每次迭代中都有不同的顶点和边缘的子集。</li>
</ol>
<p>综上, 我们认为，现代处理体系结构不太适合按大规模进行图形处理应用程序</p>
</blockquote>
<p><code>GraphPulse</code> 围绕事件驱动计算的想法, 它表示计算为event， when the value of a  vertex changes to update vertices on all outgoing edges(当顶点的值更改为更新所有传出边上的顶点时)通常生成.</p>
<h3 id="六-JetStream-Graph-Analytics-on-Streaming-Data-with-Event-Driven-Hardware-Accelerator"><a href="#六-JetStream-Graph-Analytics-on-Streaming-Data-with-Event-Driven-Hardware-Accelerator" class="headerlink" title="六 JetStream: Graph Analytics on Streaming Data with Event-Driven Hardware Accelerator"></a>六 <code>JetStream</code>: Graph Analytics on Streaming Data with Event-Driven Hardware Accelerator</h3><blockquote>
<p><code>Jetstream</code> 扩展了最近提出的基于事件的加速器，用于图形工作负载，以支持流更新。它通过事件驱动的计算模型来处理累积和单调图算法，该模型限制了访问图形顶点的较小子集，有效地重用以前的查询结果以消除冗余，并优化了增强内存存储器带宽利用率的内存访问模式。</p>
</blockquote>
<p><code>Jetstream在Kickstarter和GraphBolt</code> 软件框架上达到了约18倍的速度，这些系统在较小的批次尺寸下具有明显较高的加速度。</p>
<p>更新的 batch 大小只占原始图大小的很小一部分, 如果重头计算的话势必会带来一些不必要的冗余。逐步支持删除的问题更具挑战性，只有Kickstarter [45]，GraphBolt [26]和DZIG [25]支持它。</p>
<p>支持的算法：supports all algorithms compatible with delta-accumulative computation。</p>
<p>相对于增加，删除操作更加麻烦，我们在两个阶段中支持删除：</p>
<ol>
<li>将图形的上一个版本的逐步转换为更新图的可恢复状态</li>
<li>bringing the results to convergence again.</li>
</ol>
<h3 id="七-图处理相关论文"><a href="#七-图处理相关论文" class="headerlink" title="七 图处理相关论文"></a>七 图处理相关论文</h3><h4 id="三-how怎么解决这个问题"><a href="#三-how怎么解决这个问题" class="headerlink" title="三 how怎么解决这个问题"></a>三 how怎么解决这个问题</h4><p>GRASP augments existing cache policies to maximize reuse of hot vertices by protecting them against cache thrashing, while maintaining sufficient flexibility to capture the reuse of other vertices as needed.</p>
<ul>
<li><h4 id="论文中提出的图重排序方法的作用对象和机制以及与其他重排序方法的区别"><a href="#论文中提出的图重排序方法的作用对象和机制以及与其他重排序方法的区别" class="headerlink" title="论文中提出的图重排序方法的作用对象和机制以及与其他重排序方法的区别"></a>论文中提出的图重排序方法的作用对象和机制以及与其他重排序方法的区别</h4></li>
</ul>
<blockquote>
<p>这并未提出新的重排序方法, 而是使用已有的重排序方法,  引用下面两篇论文.</p>
<ul>
<li> [A Closer Look at Lightweight Graph Reordering](# A Closer Look at Lightweight Graph Reordering)</li>
<li>[Making caches work for graph analytics](# Making caches work for graph analytics)</li>
</ul>
</blockquote>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220511164905637.png" class="" title="image-20220511164905637">

<center>(a) Software applies vertex reordering, which segregates hot vertices at the beginning of the array. 
(b) GRASP interface exposes an ABR pair per Property Array to be configured with the bounds of the array. (c) GRASP identifies regions exhibiting different reuse based on an LLC size. </center>

<ul>
<li>Page5  论文中是如何识别热数据顶点，如何pinpoint(准确找到)热数据区域以及如何对访问进行归类；</li>
</ul>
<blockquote>
<ul>
<li><p>Conveniently, the hottest vertices are located at the beginning of the Property Array in a contiguous region thanks to the application of skew-aware reordering as seen in Fig. 3(a).  <strong>也就是说, 热顶点是通过排序得到的</strong>。</p>
</li>
<li><p>会在属性数组起始地址抓两个LLC-sized 的子区域， 一个 HIgh Reuse Region, 另一个Moderate Reuse Region.</p>
</li>
<li><p>GRASP determines this by comparing the address with the bounds of the High Reuse Region of each Property Array. </p>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220511165057988.png" class="" title="image-20220511165057988">

<p>GRASP encodes the classification result (High-Reuse, Moderate-Reuse, Low-Reuse or Default) as a 2-bit Reuse Hint, and forwards it to the LLC along with each cache request, as shown in Fig. 4, to guide specialized insertion and hit-promotion policies as described next.</p>
</li>
</ul>
</blockquote>
<h4 id="四-what-解决之后得到什么结论"><a href="#四-what-解决之后得到什么结论" class="headerlink" title="四 what 解决之后得到什么结论"></a>四 what 解决之后得到什么结论</h4><h5 id="4-1-Graph-process-framework"><a href="#4-1-Graph-process-framework" class="headerlink" title="4.1 Graph process framework"></a>4.1 Graph process framework</h5><blockquote>
<p>ligra, a widely used graph processing framework that supports both pull- and push-based computations, including switching from pull to push (and vice versa) at the start of every iteration. </p>
</blockquote>
<h5 id="4-2-Software-evaluation"><a href="#4-2-Software-evaluation" class="headerlink" title="4.2  Software evaluation"></a>4.2  Software evaluation</h5><p>论文中涉及的重排序技术</p>
<blockquote>
<p><strong>Sort</strong>  reorders vertices in the memory space by sorting them in the descending order of their degree.</p>
</blockquote>
<blockquote>
<p><strong>HubSort</strong>  segregates(隔离) hot vertices in a contiguous region by assigning them a continuous range of vertex IDs in their descending order of degree. In doing so, Hub Sorting essentially sorts all hot vertices, while largely preserving structure for the cold vertices.</p>
</blockquote>
<blockquote>
<p><strong>DBG</strong> , unlike Sort and HubSort, does not rely on sorting to segregate hot vertices. Instead, DBG coarsely(粗略的) partitions all vertices into a small number of groups based on their degree. Similar to Sort and HubSort, DBG is effective at improving spatial locality; however, unlike the other two techniques, DBG is able to largely preserve the existing graph structure.</p>
</blockquote>
<blockquote>
<p><strong>Gorder</strong> is evaluated as a representative of complex techniques. As Gorder is only available in a single-thread implementation, while reporting the net runtime of Gorder for a given dataset, we optimistically divide the reordering time by 40 (maximum number of threads supported on the server) to provide a fair comparison with skew-aware techniques whose reordering implementation is fully parallelized.</p>
</blockquote>
<h3 id="八-A-Closer-Look-at-Lightweight-Graph-Reordering"><a href="#八-A-Closer-Look-at-Lightweight-Graph-Reordering" class="headerlink" title="八 A Closer Look at Lightweight Graph Reordering"></a>八 A Closer Look at Lightweight Graph Reordering</h3><h4 id="二-why"><a href="#二-why" class="headerlink" title="二 why"></a>二 why</h4><p>To address the limitations of existing skew-aware reordering techniques, </p>
<ul>
<li> Sort achieves the maximum reduction in the cache footprint of hot vertices. However, in doing so, Sort completely decimates existing graph structure</li>
<li>Hub Sorting and Hub Clustering both classify vertices as hot or cold based on their degree and preserve the structure for cold vertices. However, in dealing with hot vertices, they resort to inefficient extremes. </li>
<li>At one extreme, Hub Sorting employs fine-grain reordering that sorts all hot vertices, destroying existing graph structure. At the other extreme, Hub Clustering does not apply any kind of reordering among hot vertices, sacrificing significant opportunity in improving cache efficiency</li>
</ul>
<p>we propose Degree-Based Grouping <a target="_blank" rel="noopener" href="https://github.com/faldupriyank/dbg">DBG</a>。</p>
<h4 id="三-how"><a href="#三-how" class="headerlink" title="三 how"></a>三 how</h4><h5 id="3-1-the-dbg-algorithm"><a href="#3-1-the-dbg-algorithm" class="headerlink" title="3.1 the dbg algorithm"></a>3.1 the dbg algorithm</h5><img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220512161706297.png" class="" title="image-20220512161706297">

<center>  DBG algorithm. Degree can be in-degree or out-degree
or sum of both.</center>

<h5 id="3-2-the-dbg-example"><a href="#3-2-the-dbg-example" class="headerlink" title="3.2 the dbg example"></a>3.2 the dbg example</h5><p> Vertex degree is shown inside the box while original vertex ID is shown below the box.</p>
<p>Vertex ordering in memory after DBG. In this example, DBG partitions vertices into three groups with degree ranges [0, 20), [20, 40) and [40, 80). DBG maintains a relative order of vertices within a group. As a result, many vertices are placed nearby the same vertices as before the reordering such as vertex sets (P4, P5, P6), (P0, P1) and (P10, P11).</p>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220512162204354.png" class="" title="image-20220512162204354">

<center>Vertex ordering in memory after DBG. </center>

<h5 id="3-3-运行作者实验"><a href="#3-3-运行作者实验" class="headerlink" title="3.3 运行作者实验"></a>3.3 运行作者实验</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1. 编译 这一步注意设置DBG_ROOT, 要不然下面第二部会出现错误</span></span><br><span class="line">export DBG_ROOT=&#x27;/home/server2/dbg&#x27;</span><br><span class="line">cd ~/dbg/apps</span><br><span class="line">make -j </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2. 下载数据集</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//el --&gt; edge list of the form (src, dst) <span class="keyword">in</span> text file</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//bel --&gt; edge list of the form (src, dst) <span class="keyword">in</span> binary file</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//vgr --&gt; binary csr format with no weight <span class="keyword">for</span> edges</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//cvgr --&gt; binary csr format with no weight <span class="keyword">for</span> edges (no self or redundant edges)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//csvgr --&gt; binary csr format with no weight <span class="keyword">for</span> edges (no self or redundant edges)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//         graph is symmetric -- so <span class="keyword">for</span> every edge (u, v) there also exist (v, u)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//cintgr --&gt; binary csr format with int weight <span class="keyword">for</span> edges (no slef or redundant edges)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">//edges are assumed to take 8 bytes and vertices (and edge weights) are assumed to take 4 bytes <span class="keyword">in</span> binary file</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">//all *gr files contain a header of 24 bytes as follows:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//number of vertices</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//number of edges</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//major number</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//minor number</span></span><br><span class="line"><span class="meta">#</span><span class="bash">//gr files are implemented based on the implementation from Galois.</span></span><br><span class="line"></span><br><span class="line">wget http://snap.stanford.edu/data/web-Google.txt.gz</span><br><span class="line">gunzip web-Google.txt.gz</span><br><span class="line">../graph-convert-utils/clean_edgelist.py web-Google.txt web-Google.el</span><br><span class="line">../graph-convert-utils/convert.sh web-Google</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3. Run individual application， 运行之前安装numactl，否则会出现错误。 安装命令 sudo apt install numactl</span></span><br><span class="line">make REORDERING_ALGO=5 DEGREE_USED_FOR_REORDERING=0 DATASET=web-Google run-PageRank</span><br></pre></td></tr></table></figure>

<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20220513152610369.png" class="" title="image-20220513152610369">



<h3 id="九-Making-caches-work-for-graph-analytics"><a href="#九-Making-caches-work-for-graph-analytics" class="headerlink" title="九 Making caches work for graph analytics"></a>九 Making caches work for graph analytics</h3><p>Hub sorting was proposed as a variant of Sort that aims to preserve some structure while reducing the cache footprint of hot vertices.</p>
<h3 id="十-Speedup-Graph-Processing-by-Graph-Ordering"><a href="#十-Speedup-Graph-Processing-by-Graph-Ordering" class="headerlink" title="十 Speedup Graph Processing by Graph Ordering"></a>十 Speedup Graph Processing by Graph Ordering</h3><p>Gorder—— the state-of-the-art structure-aware reordering technique.</p>
<h3 id="十一-如何直接对属性数组进行冷热数据分离"><a href="#十一-如何直接对属性数组进行冷热数据分离" class="headerlink" title="十一 如何直接对属性数组进行冷热数据分离"></a>十一 如何直接对属性数组进行冷热数据分离</h3><blockquote>
<p>这里对冷热数据分离是什么意思?  </p>
<p>是划分出high reuse region、Moderate （适度）Reuse Region， 不常用数据？</p>
</blockquote>
<h4 id="1-1-DBG-实验"><a href="#1-1-DBG-实验" class="headerlink" title="1.1 DBG 实验"></a>1.1 DBG 实验</h4><p>在dbg的实验中, 将el格式的图转为了cvgr –&gt; binary csr format with no weight for edges (no self or redundant edges), cintgr –&gt; binary csr format with int weight for edges (no slef or redundant edges)。pagerank算法应该是借助 ligra 使用了 cvgr格式的图。这里的话没有cache，还需要自己配置才能获取到冷热数据，因此还要看以前的grasp这个仓库。</p>
<h4 id="1-2-grasp实验"><a href="#1-2-grasp实验" class="headerlink" title="1.2 grasp实验"></a>1.2 grasp实验</h4><p>在[Domain-Specialized Cache Management for Graph Analytics ](# 图处理相关论文) 这篇论文中,  通过dbg重排序后，作者直接定义了两个cache大小的区域, 分别获取high、 moderate 区域。</p>
<h3 id="十二-Proceedings-of-the-ACM-on-Measurement-and-Analysis-of-Computing-Systems-Vol-3-No-3-Article-60"><a href="#十二-Proceedings-of-the-ACM-on-Measurement-and-Analysis-of-Computing-Systems-Vol-3-No-3-Article-60" class="headerlink" title="十二  Proceedings of the ACM on Measurement and Analysis of Computing Systems Vol. 3, No. 3, Article 60"></a>十二  Proceedings of the ACM on Measurement and Analysis of Computing Systems Vol. 3, No. 3, Article 60</h3><p>CHARACTERIZATION METRICS</p>
<h4 id="1-1-Performance-Metrics"><a href="#1-1-Performance-Metrics" class="headerlink" title="1.1 Performance Metrics"></a>1.1 Performance Metrics</h4><ol>
<li>We measure single-threaded application performance using instructions per cycle (IPC)</li>
<li>For multithreaded applications, we show parallel speedup (i.e., the single-threaded execution time divided by the parallel execution time), which accounts for synchronization overheads.</li>
<li>To quantify the memory intensity of an application, we use the number of misses per kilo-instruction (MPKI) issued by the last-level cache for that application to DRAM.</li>
</ol>
<h4 id="1-2-Parallelism-Metrics"><a href="#1-2-Parallelism-Metrics" class="headerlink" title="1.2 Parallelism Metrics"></a>1.2 Parallelism Metrics</h4><p>Prior works have used either memory-level parallelism (MLP) [26, 47, 137, 152, 181] or bank-level parallelism (BLP) to quantify the amount of parallelism across memory requests</p>
<p>MLP measures the average number of outstanding memory requests for an application, but this does not capture the amount of parallelism offered by the underlying hardware.</p>
<p>BLP measures the average number of memory requests that are actively being serviced for a single thread during a given time interval.</p>
<img src="/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/image-20221004152901298.png" class="" title="image-20221004152901298">



<h4 id="1-3-Contention-Metrics"><a href="#1-3-Contention-Metrics" class="headerlink" title="1.3 Contention Metrics"></a>1.3 Contention Metrics</h4><p>An important measure of spatial and temporal locality in memory is the row buffer hit rate, also known as row buffer locality.</p>
<p>To quantify the row hit rate, prior works count the number of row buffer hits and the number of row buffer misses, which they define as any request that does not hit in the currently-open row.</p>
<blockquote>
<p>To accurately capture row buffer locality, we introduce a new characterization methodology where we break down memory requests into: (1) row buffer hits; (2) row buffer misses, which only include misses for a DRAM request where the bank does not have any row open; and (3) row buffer conflicts, which consist of misses where another row is currently open in the bank and must be closed (<code>i.e., precharged</code>) first. Row buffer conflicts provide us with important information about how the amount of parallelism exposed by a DRAM type can limit opportunities to concurrently serve multiple memory requests, which in turn hurts performance.</p>
</blockquote>
<p>为了准确捕获行缓冲区局部性，我们引入了一种新的特征方法，将内存请求分解为：</p>
<p>（1）行缓冲区命中；</p>
<p>（2）行缓冲区错过，其中仅包括银行没有任何行打开的DRAM请求的错过； </p>
<p>（3）行缓冲冲突，其中包括当前在银行中打开另一排，必须首先关闭（即预处理）的失误。行缓冲冲突为我们提供了有关DRAM类型暴露的并行量如何限制同时服务多个内存请求的机会，这反过来又损害了性能。</p>
<h3 id="十三-Graphfire-Synergizing-Fetch-Insertion-and-Replacement-Policies-for-Graph-Analytics"><a href="#十三-Graphfire-Synergizing-Fetch-Insertion-and-Replacement-Policies-for-Graph-Analytics" class="headerlink" title="十三 Graphfire: Synergizing Fetch, Insertion, and Replacement Policies for Graph Analytics"></a>十三 Graphfire: Synergizing Fetch, Insertion, and Replacement Policies for Graph Analytics</h3><blockquote>
<p> With the goal of optimizing cache performance for graph applications, our work makes the following key observations: (i) The memory hierarchy must specialize for the problematic indirect accesses (PIAs) to alleviate their bottlenecks. (ii) To be software-agnostic, a lightweight mechanism must automatically identify PIAs, which can be achieved on a perinstruction basis. (iii) While PIAs are irregular, a subset of them have high reuse, so the LLC must retain them.</p>
</blockquote>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">okeyia</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://okeyia.github.io/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/">http://okeyia.github.io/2022/05/15/%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://okeyia.github.io" target="_blank">okeyia</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%9B%BE%E9%87%8D%E6%8E%92%E5%BA%8F/">图重排序</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/07/27/Processor_Counter_Monitor/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/okeyia/PictBed/Blogimg/202207270024935.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Processor Counter Monitor</div></div></a></div><div class="next-post pull-right"><a href="/2022/05/13/OPenMP%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/okeyia/PictBed/Blogimg/202205130900987.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">OpenMP并行编程</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%A4%84%E7%90%86%E7%9A%84%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87"><span class="toc-text">图处理的相关论文</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#STINGER-%E6%B5%81%E5%9B%BE%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="toc-text">STINGER: 流图的高性能数据结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%A4%84%E7%90%86%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD%E5%86%85%E5%AD%98%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84%E7%9A%84%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-text">图处理工作负载内存层次结构的分析与优化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80-%E5%80%9F%E9%89%B4%E7%9A%84%E6%96%B9%E5%90%91"><span class="toc-text">一 借鉴的方向</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-%E7%89%B9%E5%BE%81%E6%8F%8F%E8%BF%B0"><span class="toc-text">二 特征描述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E6%8C%87%E4%BB%A4%E7%AA%97%E5%8F%A3%E5%A4%A7%E5%B0%8F%E4%B8%8D%E6%98%AF%E9%98%BB%E7%A2%8D-MLP-%E7%9A%84%E5%9B%A0%E7%B4%A0"><span class="toc-text">2.1 指令窗口大小不是阻碍 MLP 的因素</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-2-%E8%B4%9F%E8%BD%BD%E4%BE%9D%E8%B5%96%E9%93%BE%E9%98%BB%E6%AD%A2%E5%AE%9E%E7%8E%B0%E9%AB%98-MLP"><span class="toc-text">2.1.2 负载依赖链阻止实现高 MLP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-3-%E5%9B%BE%E5%B1%9E%E6%80%A7%E6%95%B0%E6%8D%AE%E6%98%AF%E4%BE%9D%E8%B5%96%E9%93%BE%E4%B8%AD%E7%9A%84%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-text">2.1.3 图属性数据是依赖链中的消费者</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-4-%E7%A7%81%E6%9C%89-L2-%E7%BC%93%E5%AD%98%E8%A1%A8%E7%8E%B0%E5%87%BA%E5%8F%AF%E5%BF%BD%E7%95%A5%E7%9A%84%E6%80%A7%E8%83%BD%E6%95%8F%E6%84%9F%E6%80%A7%EF%BC%8C%E8%80%8C%E5%85%B1%E4%BA%AB-LLC-%E8%A1%A8%E7%8E%B0%E5%87%BA%E6%9B%B4%E9%AB%98%E7%9A%84%E6%80%A7%E8%83%BD%E6%95%8F%E6%84%9F%E6%80%A7"><span class="toc-text">2.1.4 私有 L2 缓存表现出可忽略的性能敏感性，而共享 LLC 表现出更高的性能敏感性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-5-%E5%B1%9E%E6%80%A7%E6%95%B0%E6%8D%AE%E6%98%AF-LLC-%E5%AE%B9%E9%87%8F%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8F%97%E7%9B%8A%E8%80%85"><span class="toc-text">2.1.5 属性数据是 LLC 容量的主要受益者</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-6-%E5%9B%BE%E7%BB%93%E6%9E%84%E7%BC%93%E5%AD%98%E8%A1%8C%E5%9C%A8%E6%89%80%E6%9C%89%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B8%AD%E5%85%B7%E6%9C%89%E6%9C%80%E5%A4%A7%E7%9A%84%E9%87%8D%E7%94%A8%E8%B7%9D%E7%A6%BB%E3%80%82-Graph-property-cacheline-%E5%85%B7%E6%9C%89%E6%AF%94-L2-%E7%BC%93%E5%AD%98%E6%9C%8D%E5%8A%A1%E6%9B%B4%E5%A4%A7%E7%9A%84%E9%87%8D%E7%94%A8%E8%B7%9D%E7%A6%BB"><span class="toc-text">2.1.6 图结构缓存行在所有数据类型中具有最大的重用距离。 Graph property cacheline 具有比 L2 缓存服务更大的重用距离</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89-%E6%80%BB%E7%BB%93%E4%B8%8E%E6%9C%BA%E9%81%87"><span class="toc-text">三 总结与机遇</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Graphfire%EF%BC%9A%E4%B8%BA%E5%9B%BE%E5%A4%84%E7%90%86%E5%8D%8F%E5%90%8C%E8%8E%B7%E5%8F%96%E3%80%81%E6%8F%92%E5%85%A5%E5%92%8C%E6%9B%BF%E6%8D%A2%E7%AD%96%E7%95%A5"><span class="toc-text">Graphfire：为图处理协同获取、插入和替换策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80-%E5%B7%B2%E6%9C%89%E5%B7%A5%E4%BD%9C%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">一 已有工作的问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-%E6%88%91%E4%BB%AC%E7%9A%84%E5%B7%A5%E4%BD%9C"><span class="toc-text">二 我们的工作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89-MOTIVATION"><span class="toc-text">三 MOTIVATION</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.1 内存访问模式</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#3-1-1-Infrequent-Streaming"><span class="toc-text">3.1.1 Infrequent, Streaming</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-1-2-Infrequent-Indirect"><span class="toc-text">3.1.2 Infrequent, Indirect</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-1-3-Primary-Streaming-Accesses-PSAs"><span class="toc-text">3.1.3 Primary Streaming Accesses (PSAs)</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-1-4-Primary-Indirect-Accesses"><span class="toc-text">3.1.4 Primary Indirect Accesses</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-The-Problems-with-PIAs"><span class="toc-text">3.2 The Problems with PIAs</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#3-2-1-Lack-of-Locality"><span class="toc-text">3.2.1 Lack of Locality</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-2-2-%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B9%B2%E6%89%B0"><span class="toc-text">3.2.2 访问模式之间的干扰</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-2-3-%E5%8F%98%E9%87%8F-PIA-%E9%87%8D%E7%94%A8"><span class="toc-text">3.2.3 变量 PIA 重用</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%BD%A2%E5%88%86%E6%9E%90%E7%9A%84%E9%A2%86%E5%9F%9F%E4%B8%93%E7%94%A8%E7%BC%93%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-text">用于图形分析的领域专用缓存管理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%A0%B8%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%A4%84%E7%90%86%E5%86%85%E5%AD%98%E5%B9%B2%E6%89%B0%E7%9A%84%E6%8A%80%E6%9C%AF-17-%E6%B5%99%E5%A4%A7"><span class="toc-text">多核系统中处理内存干扰的技术 17 浙大</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E6%96%B0-%E5%9F%BA%E4%BA%8E%E5%8A%A8%E6%80%81%E5%A4%9A%E5%B1%82%E6%AC%A1%E4%BC%98-%E5%85%88%E7%BA%A7%E7%9A%84%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95-DMPS"><span class="toc-text">创新: 基于动态多层次优 先级的内存访问调度算法 DMPS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80-DRAM-%E8%AE%BF%E9%97%AE%E8%BF%87%E7%A8%8B"><span class="toc-text">一 DRAM 访问过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-%E9%A1%B5%E7%AE%A1%E7%90%86%E7%AD%96%E7%95%A5"><span class="toc-text">二 页管理策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89-%E5%9C%B0%E5%9D%80%E6%98%A0%E5%B0%84%E6%9C%BA%E5%88%B6"><span class="toc-text">三 地址映射机制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%BE%9D%E8%B5%96%E6%84%9F%E7%9F%A5%E7%9A%84%E5%8A%A8%E6%80%81%E6%9C%89%E5%90%91%E5%9B%BE%E5%A4%84%E7%90%86%E5%8A%A0%E9%80%9F%E5%99%A8-20-%E5%8D%8E%E7%A7%91"><span class="toc-text">基于依赖感知的动态有向图处理加速器 20 华科</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%90%8C%E6%AD%A5%E8%BF%AD%E4%BB%A3%E6%A8%A1%E5%9E%8B"><span class="toc-text">1 同步迭代模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%BC%82%E6%AD%A5%E8%BF%AD%E4%BB%A3%E6%A8%A1%E5%9E%8B"><span class="toc-text">2 异步迭代模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E8%AE%BE%E8%AE%A1%E5%8A%A8%E6%9C%BA"><span class="toc-text">3 设计动机</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-%E5%90%8C%E6%AD%A5BSP%E8%BF%AD%E4%BB%A3%E8%BF%87%E7%A8%8B"><span class="toc-text">3.1 同步BSP迭代过程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-%E5%BC%82%E6%AD%A5%E8%BF%AD%E4%BB%A3%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2 异步迭代模型</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-%E5%AE%9E%E9%AA%8C%E9%83%A8%E5%88%86"><span class="toc-text">二 实验部分</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B-RisGraph-A-Real-Time-Streaming-System-for-Evolving-Graphs-to-Support-Sub-millisecond-Per-update-Analysis-at-Millions-Ops-s"><span class="toc-text">四 RisGraph: A Real-Time Streaming System for Evolving Graphs to Support Sub-millisecond Per-update Analysis at Millions Ops&#x2F;s</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80-motivation"><span class="toc-text">一 motivation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-text">二 实验结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text">三 相关工作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94-GraphPulse-An-Event-Driven-Hardware-Accelerator-for-Asynchronous-Graph-Processing"><span class="toc-text">五 GraphPulse: An Event-Driven Hardware Accelerator for Asynchronous Graph Processing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AD-JetStream-Graph-Analytics-on-Streaming-Data-with-Event-Driven-Hardware-Accelerator"><span class="toc-text">六 JetStream: Graph Analytics on Streaming Data with Event-Driven Hardware Accelerator</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%83-%E5%9B%BE%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87"><span class="toc-text">七 图处理相关论文</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89-how%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E8%BF%99%E4%B8%AA%E9%97%AE%E9%A2%98"><span class="toc-text">三 how怎么解决这个问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E4%B8%AD%E6%8F%90%E5%87%BA%E7%9A%84%E5%9B%BE%E9%87%8D%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E7%9A%84%E4%BD%9C%E7%94%A8%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%9C%BA%E5%88%B6%E4%BB%A5%E5%8F%8A%E4%B8%8E%E5%85%B6%E4%BB%96%E9%87%8D%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">论文中提出的图重排序方法的作用对象和机制以及与其他重排序方法的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B-what-%E8%A7%A3%E5%86%B3%E4%B9%8B%E5%90%8E%E5%BE%97%E5%88%B0%E4%BB%80%E4%B9%88%E7%BB%93%E8%AE%BA"><span class="toc-text">四 what 解决之后得到什么结论</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-Graph-process-framework"><span class="toc-text">4.1 Graph process framework</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-Software-evaluation"><span class="toc-text">4.2  Software evaluation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AB-A-Closer-Look-at-Lightweight-Graph-Reordering"><span class="toc-text">八 A Closer Look at Lightweight Graph Reordering</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-why"><span class="toc-text">二 why</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89-how"><span class="toc-text">三 how</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-the-dbg-algorithm"><span class="toc-text">3.1 the dbg algorithm</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-the-dbg-example"><span class="toc-text">3.2 the dbg example</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-3-%E8%BF%90%E8%A1%8C%E4%BD%9C%E8%80%85%E5%AE%9E%E9%AA%8C"><span class="toc-text">3.3 运行作者实验</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B9%9D-Making-caches-work-for-graph-analytics"><span class="toc-text">九 Making caches work for graph analytics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81-Speedup-Graph-Processing-by-Graph-Ordering"><span class="toc-text">十 Speedup Graph Processing by Graph Ordering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81%E4%B8%80-%E5%A6%82%E4%BD%95%E7%9B%B4%E6%8E%A5%E5%AF%B9%E5%B1%9E%E6%80%A7%E6%95%B0%E7%BB%84%E8%BF%9B%E8%A1%8C%E5%86%B7%E7%83%AD%E6%95%B0%E6%8D%AE%E5%88%86%E7%A6%BB"><span class="toc-text">十一 如何直接对属性数组进行冷热数据分离</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-DBG-%E5%AE%9E%E9%AA%8C"><span class="toc-text">1.1 DBG 实验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-grasp%E5%AE%9E%E9%AA%8C"><span class="toc-text">1.2 grasp实验</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81%E4%BA%8C-Proceedings-of-the-ACM-on-Measurement-and-Analysis-of-Computing-Systems-Vol-3-No-3-Article-60"><span class="toc-text">十二  Proceedings of the ACM on Measurement and Analysis of Computing Systems Vol. 3, No. 3, Article 60</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-Performance-Metrics"><span class="toc-text">1.1 Performance Metrics</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-Parallelism-Metrics"><span class="toc-text">1.2 Parallelism Metrics</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-Contention-Metrics"><span class="toc-text">1.3 Contention Metrics</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81%E4%B8%89-Graphfire-Synergizing-Fetch-Insertion-and-Replacement-Policies-for-Graph-Analytics"><span class="toc-text">十三 Graphfire: Synergizing Fetch, Insertion, and Replacement Policies for Graph Analytics</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-text">参考链接</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2023 By okeyia</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>